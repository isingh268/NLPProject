# -*- coding: utf-8 -*-
"""Tavily

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zPqmmW_lLoKyO8u934x37I-e7NrBEIdm
"""

import streamlit as st
from langchain_ollama import ChatOllama
from langchain.schema import HumanMessage

# Initialize Ollama Chat with Llama 3.2
def get_llama_response(prompt):
    try:
        llm = ChatOllama(model="llama3.2")
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        return f"Error generating response: {str(e)}"

# Streamlit App
def main():
    st.title("üéì Scholarship Advisor with Llama 3.2")
    st.markdown("""
    Use Llama 3.2 to fetch scholarship recommendations and provide guidance based on student profiles.
    """)

    # Student Profile Input
    st.header("üìù Student Profile")
    name = st.text_input("Full Name")
    gpa = st.slider("GPA", 0.0, 4.0, 3.0, step=0.1)
    major = st.text_input("Major (e.g., Computer Science, Biology)")
    school_year = st.selectbox(
        "School Year",
        ["High School Senior", "College Freshman", "Sophomore", "Junior", "Senior", "Graduate Student"]
    )
    financial_need = st.selectbox("Financial Need?", ["Yes", "No"])
    ethnicity = st.text_input("Ethnicity (e.g., Hispanic, Asian)")
    residence_state = st.text_input("State of Residence (e.g., California)")
    causes = st.multiselect(
        "Causes or Interests",
        ["Community Service", "Sustainability", "Social Justice", "Diversity", "STEM", "Arts"]
    )

    # Generate Recommendations
    if st.button("Get Recommendations"):
        # Create Prompt for Llama
        prompt = (
            f"Student Profile:\n"
            f"Name: {name}\n"
            f"GPA: {gpa}\n"
            f"Major: {major}\n"
            f"School Year: {school_year}\n"
            f"Financial Need: {financial_need}\n"
            f"Ethnicity: {ethnicity}\n"
            f"Residence State: {residence_state}\n"
            f"Causes: {', '.join(causes)}\n\n"
            f"Generate scholarship recommendations and next steps for this student."
        )

        with st.spinner("Fetching recommendations..."):
            response = get_llama_response(prompt)

        st.success("Here are your scholarship recommendations:")
        st.write(response)

if __name__ == "__main__":
    main()
